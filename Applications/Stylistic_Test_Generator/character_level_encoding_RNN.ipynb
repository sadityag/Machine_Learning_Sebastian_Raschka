{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ab8453d",
   "metadata": {},
   "source": [
    "# Text Generation In Style\n",
    "\n",
    "This project will focus on absorbing the text of \"The Mysterious Island\" authored by Jules Verne, and using an RNN to generate new text that is similar in style to it. This can be generalized beyond this text fairly easily, with similar architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f509b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84774783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6167a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Length: 1130779\n",
      "Unique Characters: 86\n",
      "Shape of Encoding: (1130779,)\n",
      "Example encoding: Towns were -> [46 67 75 66 71  1 75 57 70 57]\n",
      "Reverse process: [35 45 38 27 40 30] -> ISLAND\n"
     ]
    }
   ],
   "source": [
    "# Use numpy to open the text and encode it\n",
    "with open('The_Mysterious_Island_Jules_Verne.txt', 'r', encoding='utf8') as fp:\n",
    "    text = fp.read()\n",
    "\n",
    "start_idx = text.find('THE MYSTERIOUS ISLAND')\n",
    "end_idx = text.find('End of the Project Gutenberg')\n",
    "\n",
    "text = text[start_idx: end_idx]\n",
    "char_set = set(text)\n",
    "\n",
    "print(f'Total Length: {len(text)}')\n",
    "print(f'Unique Characters: {len(char_set)}')\n",
    "\n",
    "# Sort characters and create a map to integers\n",
    "sorted_chars = sorted(char_set)\n",
    "char_arr = np.array(sorted_chars)\n",
    "char2int = {ch: i for i, ch in enumerate(sorted_chars)}\n",
    "\n",
    "# Encode the text using the map\n",
    "text_encoded = np.array([char2int[ch] for ch in text], dtype=np.int32)\n",
    "# Demonstration\n",
    "print(f'Shape of Encoding: {text_encoded.shape}')\n",
    "print(f'Example encoding: {text[1080: 1090]} -> {text_encoded[1080: 1090]}')\n",
    "print(f'Reverse process: {text_encoded[15: 21]} -> {text[15: 21]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438ce3cf",
   "metadata": {},
   "source": [
    "## Sequencing Strategy\n",
    "\n",
    "The strategy will be to use contiguous chunks of text as inputs, say the first $n$ words, where the output will be a sequence of $n$ words, shifted by one index, from 1 to $n+1$ instead of 0 to $n$. This requires the network to predict a word/character ahead based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb70903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'THE MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOU'\n",
      "Target: 'HE MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS'\n",
      "\n",
      "Input: 'HE MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS'\n",
      "Target: 'E MYSTERIOUS ISLAND ***\\n\\nTHE MYSTERIOUS '\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadit\\AppData\\Local\\Temp\\ipykernel_22928\\754645450.py:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  seq_data = TextDataset(text_chunks=torch.tensor(text_chunks))\n"
     ]
    }
   ],
   "source": [
    "# This is \"n\"\n",
    "seq_length = 40\n",
    "chunk_size = seq_length + 1\n",
    "\n",
    "# Form overlapping chunks from the text\n",
    "text_chunks = [text_encoded[i: i + chunk_size] for i in range(len(text_encoded) - chunk_size + 1)]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text_chunks):\n",
    "        self.text_chunks = text_chunks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text_chunks)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_chunk = self.text_chunks[idx]\n",
    "        return text_chunk[:-1].long(), text_chunk[1:].long()\n",
    "\n",
    "seq_data = TextDataset(text_chunks=torch.tensor(text_chunks))\n",
    "\n",
    "for i, (seq, tgt) in enumerate(seq_data):\n",
    "    print(f'Input: {repr(''.join(char_arr[seq]))}')\n",
    "    print(f'Target: {repr(''.join(char_arr[tgt]))}\\n')\n",
    "    if i:\n",
    "        break\n",
    "\n",
    "batch_size = 64\n",
    "torch.manual_seed(42)\n",
    "seq_dl = DataLoader(seq_data, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31cea2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embedding): Embedding(86, 256)\n",
      "  (rnn): LSTM(256, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=86, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_dim)\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "        self.rnn = nn.LSTM(input_size=embed_dim, hidden_size=rnn_hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, out_features=vocab_size)\n",
    "    \n",
    "    def forward(self, x, hidden, cell):\n",
    "        out = self.embedding(x).unsqueeze(1)\n",
    "        out, (hidden, cell) = self.rnn(out, (hidden, cell))\n",
    "        out = self.fc(out).reshape(out.size(0), -1)\n",
    "        return out, hidden, cell\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        cell = torch.zeros(1, batch_size, self.rnn_hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "vocab_size = len(char_arr)\n",
    "embed_dim = 256\n",
    "rnn_hidden_size = 512\n",
    "\n",
    "torch.manual_seed(42)\n",
    "model = RNN(vocab_size=vocab_size, embed_dim=embed_dim, rnn_hidden_size=rnn_hidden_size)\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ca95d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 4.4663\n",
      "Epoch 500 loss: 1.4206\n",
      "Epoch 1000 loss: 1.2193\n",
      "Epoch 1500 loss: 1.2246\n",
      "Epoch 2000 loss: 1.2146\n",
      "Epoch 2500 loss: 1.2477\n",
      "Epoch 3000 loss: 1.1983\n",
      "Epoch 3500 loss: 1.1828\n",
      "Epoch 4000 loss: 1.2234\n",
      "Epoch 4500 loss: 1.1991\n",
      "Epoch 5000 loss: 1.2375\n",
      "Epoch 5500 loss: 1.1971\n",
      "Epoch 6000 loss: 1.2376\n",
      "Epoch 6500 loss: 1.1642\n",
      "Epoch 7000 loss: 1.1411\n",
      "Epoch 7500 loss: 1.1703\n",
      "Epoch 8000 loss: 1.2136\n",
      "Epoch 8500 loss: 1.1398\n",
      "Epoch 9000 loss: 1.1769\n",
      "Epoch 9500 loss: 1.1582\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    hidden, cell = model.init_hidden(batch_size=batch_size)\n",
    "    hidden, cell = hidden.to(device), cell.to(device)\n",
    "    \n",
    "    seq_batch, target_batch = next(iter(seq_dl))\n",
    "    seq_batch, target_batch = seq_batch.to(device), target_batch.to(device)    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(seq_length):\n",
    "        pred, hidden, cell = model(seq_batch[:, c], hidden, cell)\n",
    "        loss += loss_fn(pred, target_batch[:, c])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    loss = loss.item() / seq_length\n",
    "\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch} loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d2472d",
   "metadata": {},
   "source": [
    "## Generating New Text\n",
    "\n",
    "The RNN that has been trained returns the logits (of size equivalent to our character set) for each unique character, which can easily be converted to probabilities. The value with the highest logit/probability corresponds to the prediction of the next character in the sequence. In order to generate variation in the text produced, it is possible to instead sample from the outputs using the probabilities to form a distribution over characters. This can be acheived by using the Categorical sub-class from torch.distributions.categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da985ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, starting_str, len_generated_text=500, scale_factor=1.0):\n",
    "    encoded_input = torch.tensor([char2int[s] for s in starting_str])\n",
    "    encoded_input = torch.reshape(encoded_input, (1, -1)).to(device)\n",
    "    generated_str = starting_str\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    hidden, cell = model.init_hidden(1)\n",
    "    hidden, cell = hidden.to(device), cell.to(device)\n",
    "\n",
    "    for ctr in range(len(starting_str) - 1):\n",
    "        _, hidden, cell = model(encoded_input[:, ctr].view(1), hidden, cell)\n",
    "    \n",
    "    last_char = encoded_input[:, -1]\n",
    "\n",
    "    for i in range(len_generated_text):\n",
    "        logits, hidden, cell = model(last_char.view(1), hidden, cell)\n",
    "        logits = torch.squeeze(logits, 0)\n",
    "\n",
    "        # The higher the scale factor, the more the largest probabilities dominate\n",
    "        # High scale factor = sampling more of the large logits\n",
    "        scaled_logits = logits * scale_factor\n",
    "\n",
    "        m = Categorical(logits=scaled_logits)\n",
    "        last_char = m.sample()\n",
    "        generated_str += str(char_arr[last_char])\n",
    "    \n",
    "    return generated_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22462d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale Factor of 1.0\n",
      "The island.\n",
      "\n",
      "The stranger had not orden to get his bottle glass, to a cinde bub. At enant sea.”\n",
      "\n",
      "“The brig, for the world kyut in the work.”\n",
      "\n",
      "“‘others to hight back of cisadly sharponed, measured in this aspen to time by latitudes large infligence, through the subman. Lastly. In the west ever of the wingtones of the inhabited cord, or\n",
      "bounding to use the new longipal to beat.\n",
      "\n",
      "The unfortunate disappearance.”\n",
      "\n",
      "“Alas. The attackmen performation an islet, you at each said,--\n",
      "\n",
      "“He tooly they led the little.”\n",
      "\n",
      "\n",
      "Higher Scale Factor: 3.0\n",
      "The island was accompanied him to the sea had been so many very slippered to Granite House the sea had been provided a complish to the sea had been transport. The next day, the 29th of October to the sea which had been easy to form the sun would be seen that the\n",
      "colonists were all reached the coast was then took the wind beam to the northeast was about to make a few minutes were to be feared the corral, and the settlers were considered the colonists, which had been a south companions. The colonists were a\n",
      "\n",
      "Lower Scale Factor: 0.5\n",
      "The island was a man who had been able to get for the northeast, and the settlers had been traversed the sailor, who was already to the sea, and the settlers were all ready to the sea, and the windows and the limits of the Mercy, while the colonists, and the colonists were not a convicts were all reached the colonists were also that was not a man who had been seen the sailor and his companions were to be a sudden and seat in the sea. The engineer had not been able to get a strong of the little band of the\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"Scale Factor of 1.0\")\n",
    "print(sample(model, starting_str='The island'))\n",
    "print()\n",
    "print('Higher Scale Factor: 3.0')\n",
    "print(sample(model, starting_str='The island', scale_factor=3.0))\n",
    "\n",
    "print()\n",
    "print('Lower Scale Factor: 0.5')\n",
    "print(sample(model, starting_str='The island', scale_factor=3.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataexercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
