{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63b9ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to deal with the same dataset in a more compute friendly manner\n",
    "# These methods will allow us to create tools that can be deployed online\n",
    "# We will use an SGD classifier to process in small batches\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "rnd_seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c8ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"I was taken to this film by a friend and was sceptical about a Swedish film with subtitles. However, I thoroughly enjoyed every minute of this beautiful film. The unnecessary cruelty that man is capable of was portrayed confidently without overwhelming images - although animal lovers may have to shield their eyes for a brief couple of seconds somewhere during the first 10 minutes. A traditional story of humility versus brutality and hope versus tragedy was illustrated from a satisfyingly fresh angle using a spectrum of characters with very natural flaws and features. I particularly liked how the film managed to address multiple aspects of hypocritical human behaviour that concern bias, discrimination and sanctimonious pretence. An absolute gem of a film that I will promote to all who will listen.\"',\n",
       " 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "def tokenizer(txt):\n",
    "    txt = re.sub(r'<[^>]*>', '', txt) # Remove all breaks, formatting etc\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', txt) # Sift out emojis to add to the end\n",
    "    txt = re.sub(r'[\\W]+', ' ', txt.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return [w for w in txt.split() if w not in stop] # Remove nuisance stop words\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            txt, label = line[:-3], int(line[-2]) # Read off the review and label in the csv\n",
    "            yield txt, label # return a generator object\n",
    "\n",
    "# Test\n",
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f96ca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# We use a Hashing vectorizer: converts a collection of documents into a sparse numpy matrix\n",
    "# This is similar to the Count Vectorizer\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         norm='l2',\n",
    "                         ngram_range=(1, 1),\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log_loss', random_state=rnd_seed)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c6fb12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:18<00:00,  2.42it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_batches = 45\n",
    "batch_size = 1000\n",
    "\n",
    "pbar = tqdm(total=n_batches)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "for _ in range(n_batches):\n",
    "    X_train, y_train = get_minibatch(doc_stream=doc_stream, size=batch_size)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f3fa35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "test_size = 50000 - (n_batches * batch_size)\n",
    "X_test, y_test = get_minibatch(doc_stream, size=test_size)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f\"Test accuracy: {clf.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424cfdf",
   "metadata": {},
   "source": [
    "This was slightly weaker, with just over an 86% accuracy. However, we did not perform the stemming operations, gridsearching, or any real other optimization. This was a simple batch update. However, for a few percent loss in accuracy, the entire code ran in under a minute, in stark contrast to sentiment_analysis.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataexercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
