{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64c2e5a",
   "metadata": {},
   "source": [
    "Given a bag-of-words matrix as input, LDA decomposes it into two new matrices:\n",
    "• A document-to-topic matrix\n",
    "• A word-to-topic matrix\n",
    "LDA decomposes the bag-of-words matrix in such a way that if we multiply those two matrices to￾gether, we will be able to reproduce the input, the bag-of-words matrix, with the lowest possible error. \n",
    "In practice, we are interested in those topics that LDA found in the bag-of-words matrix. The only \n",
    "downside may be that we must define the number of topics beforehand—the number of topics is a \n",
    "hyperparameter of LDA that has to be specified manually.\n",
    "\n",
    "- Sebastian Raschka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec16b9",
   "metadata": {},
   "source": [
    "# Statistical Foundation of Latent Dirichlet Allocation\n",
    "\n",
    "## Model Setup\n",
    "\n",
    "**Variables:**\n",
    "- $w_{d,n}$ = n-th word in document d\n",
    "- $z_{d,n}$ = topic assignment for word $w_{d,n}$\n",
    "- $\\boldsymbol{\\theta}_d$ = topic distribution for document d (K-dimensional)\n",
    "- $\\boldsymbol{\\phi}_k$ = word distribution for topic k (V-dimensional)\n",
    "- $\\boldsymbol{\\alpha}, \\boldsymbol{\\beta}$ = Dirichlet hyperparameters\n",
    "\n",
    "## Generative Process\n",
    "\n",
    "For each topic $k$:\n",
    "$$\\boldsymbol{\\phi}_k \\sim \\text{Dirichlet}(\\boldsymbol{\\beta})$$\n",
    "\n",
    "For each document $d$:\n",
    "$$\\boldsymbol{\\theta}_d \\sim \\text{Dirichlet}(\\boldsymbol{\\alpha})$$\n",
    "\n",
    "For each word position $n$ in document $d$:\n",
    "$$z_{d,n} \\sim \\text{Multinomial}(\\boldsymbol{\\theta}_d)$$\n",
    "$$w_{d,n} \\sim \\text{Multinomial}(\\boldsymbol{\\phi}_{z_{d,n}})$$\n",
    "\n",
    "## Joint Distribution\n",
    "\n",
    "$$P(\\mathbf{w},\\mathbf{z},\\boldsymbol{\\Theta},\\boldsymbol{\\Phi}|\\boldsymbol{\\alpha},\\boldsymbol{\\beta}) = \\prod_{k=1}^K \\text{Dir}(\\boldsymbol{\\phi}_k|\\boldsymbol{\\beta}) \\prod_{d=1}^D \\text{Dir}(\\boldsymbol{\\theta}_d|\\boldsymbol{\\alpha}) \\prod_{d=1}^D \\prod_{n=1}^{N_d} \\theta_{d,z_{d,n}} \\phi_{z_{d,n},w_{d,n}}$$\n",
    "\n",
    "## Inference Problem\n",
    "\n",
    "We want the posterior:\n",
    "$$P(\\mathbf{z},\\boldsymbol{\\Theta},\\boldsymbol{\\Phi}|\\mathbf{w},\\boldsymbol{\\alpha},\\boldsymbol{\\beta}) = \\frac{P(\\mathbf{w},\\mathbf{z},\\boldsymbol{\\Theta},\\boldsymbol{\\Phi}|\\boldsymbol{\\alpha},\\boldsymbol{\\beta})}{P(\\mathbf{w}|\\boldsymbol{\\alpha},\\boldsymbol{\\beta})}$$\n",
    "\n",
    "The denominator is intractable, requiring approximation methods.\n",
    "\n",
    "## Collapsed Gibbs Sampling\n",
    "\n",
    "By integrating out $\\boldsymbol{\\Theta}$ and $\\boldsymbol{\\Phi}$, we sample topic assignments directly. The conditional probability for assigning topic $k$ to word $w_{d,n}$ is:\n",
    "\n",
    "$$P(z_{d,n}=k | \\mathbf{z}_{-d,n}, \\mathbf{w}, \\boldsymbol{\\alpha}, \\boldsymbol{\\beta}) \\propto \\frac{n_{d,k}^{-d,n} + \\alpha_k}{\\sum_{k'=1}^K (n_{d,k'}^{-d,n} + \\alpha_{k'})} \\cdot \\frac{n_{k,w_{d,n}}^{-d,n} + \\beta_{w_{d,n}}}{\\sum_{w'=1}^V (n_{k,w'}^{-d,n} + \\beta_{w'})}$$\n",
    "\n",
    "Where:\n",
    "- $n_{d,k}^{-d,n}$ = count of topic $k$ in document $d$ (excluding current word)\n",
    "- $n_{k,w}^{-d,n}$ = count of word $w$ in topic $k$ (excluding current word)\n",
    "\n",
    "## Parameter Estimation\n",
    "\n",
    "After sampling, estimate parameters:\n",
    "\n",
    "**Document-topic distributions:**\n",
    "$$\\hat{\\theta}_{d,k} = \\frac{n_{d,k} + \\alpha_k}{\\sum_{k'=1}^K (n_{d,k'} + \\alpha_{k'})}$$\n",
    "\n",
    "**Topic-word distributions:**\n",
    "$$\\hat{\\phi}_{k,w} = \\frac{n_{k,w} + \\beta_w}{\\sum_{w'=1}^V (n_{k,w'} + \\beta_{w'})}$$\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "1. **Initialize:** Randomly assign topics to all words\n",
    "2. **For each iteration:**\n",
    "   - For each word $w_{d,n}$:\n",
    "     - Remove current topic assignment\n",
    "     - Sample new topic using conditional probability above\n",
    "     - Update count matrices\n",
    "3. **Estimate:** Compute $\\hat{\\boldsymbol{\\theta}}$ and $\\hat{\\boldsymbol{\\phi}}$ from final counts\n",
    "\n",
    "## Model Evaluation\n",
    "\n",
    "**Perplexity:**\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{\\sum_{d} \\sum_{n} \\log P(w_{d,n}|\\mathbf{w}_{train})}{\\sum_{d} N_d}\\right)$$\n",
    "\n",
    "**Topic Coherence:** Measures semantic similarity of top words within topics:\n",
    "$$C(k) = \\sum_{i=1}^{M-1} \\sum_{j=i+1}^{M} \\log \\frac{P(w_i^{(k)}, w_j^{(k)}) + \\epsilon}{P(w_i^{(k)}) P(w_j^{(k)})}$$\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "- **$\\alpha_k < 1$:** Sparse document-topic distributions (documents focus on fewer topics)\n",
    "- **$\\beta_w < 1$:** Sparse topic-word distributions (topics use fewer distinctive words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "add60c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "448bcff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=0.1, # maximum document frequency - arbitrary choice\n",
    "                        max_features=5000) # maximum number of words to consider (taken to be the most frequent occuring ones) - arbitrary\n",
    "\n",
    "X = count.fit_transform(df['review'].values)\n",
    "y = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66635ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=10, # number of topics learned. It is costly to learn topics\n",
    "                                random_state=1,\n",
    "                                learning_method='batch') # Train on all avaialble data. 'online' would be equivalent to mini-batch training\n",
    "\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aed37cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n",
      "(50000, 10)\n",
      "Topic 0: script, worst, poor, minutes, production\n",
      "Topic 1: original, series, episode, worst, stupid\n",
      "Topic 2: family, book, kids, children, school\n",
      "Topic 3: horror, ending, scary, original, suspense\n",
      "Topic 4: music, beautiful, performance, excellent, wonderful\n",
      "Topic 5: woman, wife, father, mother, husband\n",
      "Topic 6: war, series, american, documentary, game\n",
      "Topic 7: john, role, played, plays, michael\n",
      "Topic 8: guy, action, effects, dead, looks\n",
      "Topic 9: comedy, laugh, humor, jokes, fun\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)\n",
    "print(X_topics.shape)\n",
    "\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "n_top_words = 5\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {idx}: {', '.join(feature_names[i] for i in topic.argsort()[:-n_top_words - 1: -1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df0fc4",
   "metadata": {},
   "source": [
    "Looks like the topics are:\n",
    "0) Bad movies\n",
    "1) Sequels, series, shows\n",
    "2) Kids movies\n",
    "3) Horror movies\n",
    "4) Art movies\n",
    "5) Family movies\n",
    "6) War movies\n",
    "7) Unclear - possibly actors?\n",
    "8) Action movies\n",
    "9) Comedy movies\n",
    "\n",
    "Let's test out the comedy movies topics to see what falls out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a9400ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "War movie #0:\n",
      "The first 2 parts seek to reduce to absurdity the rise of wasteful wars and rule by nationalist barbarians. The 3rd part speculates that progress and exploration toward the moon and beyond is the key to ensuring a meaningful use of human talents and resources. It has speeches that some viewers dismi ...\n",
      "War movie #1:\n",
      "There is an episode of The Simpsons which has a joke news report referring to an army training base as a \"Killbot Factory\". Here the comment is simply part of a throwaway joke, but what Patricia Foulkrod's documentary does is show us, scarily, that it is not that far from the truth. After World War  ...\n",
      "War movie #2:\n",
      "One of the best documentaries released in recent years. Some points...<br /><br />1. Hugo Chavez was elected Venezuela's president in 1998, his support largely coming from the poorer regions of Venezuela.<br /><br />2. In 2002, a coup briefly deposed Chavez. At the time, Irish filmmakers Kim Bartley ...\n",
      "War movie #3:\n",
      "There are no reasons of taking this documentary serious and there are four reasons for that: <br /><br />1) The people who made this documentary (including the director and the producer) are Serbs or of Serbian origin, therefore the criteria of neutrality fails. For instance, they mentioned that the ...\n",
      "War movie #4:\n",
      "H.G. Wells in 1936 was past his prime and the books of his that will survive were long gone by. He was coming to the end of his life and he was confronted to his dream gone sour. At the very beginning of the 20th century he defended the idea that the world was doomed because the evolution of species ...\n"
     ]
    }
   ],
   "source": [
    "war = X_topics[:, 6].argsort()[::-1]\n",
    "for idx, movie_idx in enumerate(war[:5]):\n",
    "    print(f\"War movie #{idx}:\")\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a4a12",
   "metadata": {},
   "source": [
    "Sounds like we got war based movies or something in the apocalypse category, which can bleed into it quite easily!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataexercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
