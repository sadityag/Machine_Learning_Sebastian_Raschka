{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24acadd3",
   "metadata": {},
   "source": [
    "This project dataset comes from https://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b76c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "rnd_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e602a422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = {'pos': 1, 'neg': 0}\n",
    "# pbar = tqdm(total=50000)\n",
    "\n",
    "# data = []\n",
    "# # df = pd.DataFrame()\n",
    "\n",
    "# for s in ('test', 'train'):\n",
    "#     for l in ('pos', 'neg'):\n",
    "#         path = os.path.join(basepath, s, l)\n",
    "#         for file in sorted(os.listdir(path)):\n",
    "#             with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "#                 text = infile.read()\n",
    "#             # df = pd.concat([df, pd.DataFrame([[text, labels[l]]])], ignore_index=True)\n",
    "#             data.append([text, labels[l]])\n",
    "#             pbar.update(1)\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# df.columns = ['review', 'sentiment']\n",
    "\n",
    "# # store as a csv\n",
    "\n",
    "# np.random.seed(rnd_seed)\n",
    "# df = df.reindex(np.random.permutation(df.index))\n",
    "# df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a712b200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 781.4+ KB\n",
      "None\n",
      "                                              review  sentiment\n",
      "0  I was taken to this film by a friend and was s...          1\n",
      "1  This trash version of `Romeo and Juliet' passe...          1\n",
      "2  There is a lot to like in this film, despite i...          1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "print(df.info())\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231a7c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of doc strings is: 6\n",
      "{'the': 15, 'quick': 11, 'brown': 2, 'fox': 4, 'jumps': 6, 'over': 9, 'lazy': 7, 'dog': 3, 'sun': 13, 'shines': 12, 'brightly': 1, 'and': 0, 'weather': 17, 'is': 5, 'sweet': 14, 'one': 8, 'plus': 10, 'two': 16}\n",
      "18\n",
      "[[0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0]\n",
      " [1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1]\n",
      " [0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 2 0 0]\n",
      " [1 0 0 0 0 2 0 0 4 0 2 0 0 0 0 0 2 0]]\n",
      "(6, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# This allows us to create a bag of words to treat as a feature\n",
    "count = CountVectorizer()\n",
    "\n",
    "docs = np.array(['The quick brown fox',\n",
    "                 'jumps over the lazy dog',\n",
    "                 'The sun shines brightly',\n",
    "                 'and the weather is sweet',\n",
    "                 'The quick brown fox jumps over the lazy dog',\n",
    "                 'One plus one is two and two is one plus one'])\n",
    "\n",
    "print(f\"The number of doc strings is: {len(docs)}\")\n",
    "\n",
    "bag_of_words = count.fit_transform(docs)\n",
    "\n",
    "print(count.vocabulary_)\n",
    "print(len(count.vocabulary_))\n",
    "print(bag_of_words.toarray())\n",
    "print(bag_of_words.toarray().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20953cda",
   "metadata": {},
   "source": [
    "There are 18 words included in total amongst the 6 documents (doc strings). Each feature vector has an index corresponding to a word (index 0 is 'and' in this case, for example, and index 15 is 'the'), and the number at that index is the frequency of the word in the document.\n",
    "\n",
    "This is an example of 1-gram representation. Each feature index corresponds to a single word. An n-gram representation encodes n consecutive words. For example, the n-gram decomposition of 'I like cats too' is 'I like', 'like cats', 'cats too' where the 1-gram representation is 'I', 'like', 'cats', 'too'.\n",
    "\n",
    "The CountVectorizer can deal with this as well - it just needs to be initialized with CountVectorizer(ngram_range=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516e82af",
   "metadata": {},
   "source": [
    "A useful trick to avoid overloading the features extracted with words that don't hold much discrimantory information, such as 'and' and 'the' is known as term frequency inverse document frequency (tf-idf). This is the prodcut of the term frequency and the inverse document frequency. We have already seen the term frequency before: $tf(t, d)$ is the frequency that term $t$ appears in the document $d$ (exactly what was encoded above). For example, $tf($'plus', 'One plus one is two and two is one plus one'$) = 2$. The document frequency is the number of documents $d$ that contain the term $t$ and is denoted $df(t)$. In the example above, $df('and') = 2$.\n",
    "\n",
    "With a total of $n_d$ documents, the inverse document frequency is given by $$idf(t) = \\log \\frac{n_d}{1 + df(t)},$$ and the tf-idf is simply $$tf\\text{-}idf(t, d) = tf(t, d)\\times idf(t)$$.\n",
    "\n",
    "The tf-idf is used to weight the words, and clearly goes down for a word that appears a lot. Scikit-learn has a transformer for this. It also has a smoothing option, where $$idf(t) = \\log \\frac{1 + n_d}{1 + df(t)}$$ to assign an idf of 0 to words that appear in all documents. This also does $$tf\\text{-}idf(t, d) = tf(t, d)\\times (idf(t) + 1),$$ in order to prevent the weight from going to 0 completely.\n",
    "\n",
    "It is typical to normalize the feature vectors before the $tf\\text{-}idf$ weighting, but sklearn does this post fact.\n",
    "\n",
    "As an effort to understand the calculation it will undertake, let's take up a calculation ourselves. $$tf\\text{-}idf('the', d_2) = tf('the', 'jumps\\, over\\, the\\, lazy\\, dog') \\times (idf('the') + 1) = 1 \\times (1 + \\log \\frac{1 + 6}{1 + 5}) = 1 + \\log \\frac{7}{6}.$$ This takes the place of one index, in one feature vector. Once the entire feature vector is calculated thus, we normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed9344c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 1 1 0 1 0 0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0]\n",
      " [1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 1]\n",
      " [0 0 1 1 1 0 1 1 0 1 0 1 0 0 0 2 0 0]\n",
      " [1 0 0 0 0 2 0 0 4 0 2 0 0 0 0 0 2 0]]\n",
      "[[0.   0.   0.54 0.   0.54 0.   0.   0.   0.   0.   0.   0.54 0.   0.\n",
      "  0.   0.34 0.   0.  ]\n",
      " [0.   0.   0.   0.48 0.   0.   0.48 0.48 0.   0.48 0.   0.   0.   0.\n",
      "  0.   0.3  0.   0.  ]\n",
      " [0.   0.55 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.55 0.55\n",
      "  0.   0.28 0.   0.  ]\n",
      " [0.43 0.   0.   0.   0.   0.43 0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.53 0.27 0.   0.53]\n",
      " [0.   0.   0.34 0.34 0.34 0.   0.34 0.34 0.   0.34 0.   0.34 0.   0.\n",
      "  0.   0.43 0.   0.  ]\n",
      " [0.16 0.   0.   0.   0.   0.31 0.   0.   0.76 0.   0.38 0.   0.   0.\n",
      "  0.   0.   0.38 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2', # Normalizes with an l2 norm post-weighting\n",
    "                         smooth_idf=True)\n",
    "\n",
    "print(bag_of_words.toarray())\n",
    "print(np.round(tfidf.fit_transform(bag_of_words).toarray(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3be031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to the first group, my vote is eight.<br /><br />\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[1, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51683011",
   "metadata": {},
   "source": [
    "In need of cleanup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cecc835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' to the first group my vote is eight '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(txt):\n",
    "    txt = re.sub(r'<[^>]*>', '', txt) # removes the extra text pieces\n",
    "    emoticons = re.findall(r'(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', txt) # seeks emoticons and temporarily stores them\n",
    "    txt = (re.sub(r'[\\W]+', ' ', txt.lower()) + # removes all non-word charactes\n",
    "           ' '.join(emoticons).replace('-', '')) # moves all emoticons to the end of the string, and removes noses from the emoticon faces for consistency\n",
    "    return txt\n",
    "\n",
    "preprocess(df.loc[1, 'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6709c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly the preprocessing works. Let's now dive into it:\n",
    "df['review'] = df['review'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52798adf",
   "metadata": {},
   "source": [
    "We need to tokenize the data next. This will be where we create arrays of words by some judicious choice of splitting and modification. The basic tokenizer just removes all spaces and punctuations using the split function.\n",
    "\n",
    "Now, for recognizing the words themselves. As it stands, 'love', 'loveable', 'loving', etc would all possibly end up as significantly different features, despite similar-to-identical meanings they bring to the context. Remedying this requires something called stemming.\n",
    "\n",
    "We will use the NLTK library, which contains the Porter stemmer.\n",
    "Essentially, the Porter stemmer classifies every character in a given token as either a consonant (c) or vowel (v), grouping subsequent consonants as C and subsequent vowels as V. The stemmer thus represents every word token as a combination of consonant and vowel groups. Once enumerated this way, the stemmer runs each word token through a list of rules that specify ending characters to remove according to the number of vowel-consonant groups in a token. Because English itself follows general but not absolute lexical rules, the Porter stemmer algorithmâ€™s systematic criterion for determining suffix removal can return errors.\n",
    "\n",
    "This allows us to effectively tokenize into a reduced, and more meaningful subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff190b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tokenization: ['workers', 'must', 'keep', 'working', 'away', 'at', 'their', 'work', 'and', 'thus', 'they', 'do']\n",
      "Unique tokens in basic: ['and', 'at', 'away', 'do', 'keep', 'must', 'their', 'they', 'thus', 'work', 'workers', 'working']\n",
      "Porter tokenziation: ['worker', 'must', 'keep', 'work', 'away', 'at', 'their', 'work', 'and', 'thu', 'they', 'do']\n",
      "Unique tokens in porter: ['and', 'at', 'away', 'do', 'keep', 'must', 'their', 'they', 'thu', 'work', 'worker']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def basic_tokenizer(txt):\n",
    "    return txt.split()\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def porter_tokenizer(txt):\n",
    "    return [porter.stem(word) for word in txt.split()]\n",
    "\n",
    "example_text_block = \"workers must keep working away at their work and thus they do\"\n",
    "\n",
    "print(f\"Basic tokenization: {basic_tokenizer(example_text_block)}\")\n",
    "print(f\"Unique tokens in basic: {sorted(set(basic_tokenizer(example_text_block)))}\")\n",
    "print(f\"Porter tokenziation: {porter_tokenizer(example_text_block)}\")\n",
    "print(f\"Unique tokens in porter: {sorted(set(porter_tokenizer(example_text_block)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c3a9a",
   "metadata": {},
   "source": [
    "This doesn't deal with everything - there are many nuisance words like 'and', 'at', etc. These don't add much information to the context. In the context of NLP, they are known as _stop words_. The NLTK library can help us deal with these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3368f81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sadit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['worker', 'must', 'keep', 'work', 'away', 'work', 'thu']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "[w for w in porter_tokenizer(example_text_block) if w not in stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d4ff9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['review'].values\n",
    "y = df['sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=rnd_seed, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a1affc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we move on to creating a classification model\n",
    "# Optimize a logistic regressor with GridSearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{\n",
    "    'vect__ngram_range': [(1, 1)],\n",
    "    'vect__stop_words': [None],\n",
    "    'vect__tokenizer': [basic_tokenizer, porter_tokenizer],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__C': [1.0, 10.0]\n",
    "},\n",
    "{\n",
    "    'vect__ngram_range': [(1, 1)],\n",
    "    'vect__stop_words': [stop, None],\n",
    "    'vect__tokenizer': [basic_tokenizer],\n",
    "    'vect__use_idf': [False],\n",
    "    'vect__norm': [None],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__C': [1.0, 10.0]\n",
    "}]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                    ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(estimator=lr_tfidf,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "988dc6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadit\\miniconda3\\envs\\dataexercises\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function basic_tokenizer at 0x00000283E5C532E0>}\n"
     ]
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)\n",
    "\n",
    "print(gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d08325a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.988\n",
      "Test accuracy: 0.899\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "\n",
    "print(f\"Training accuracy: {clf.score(X_train, y_train):.3f}\")\n",
    "print(f\"Test accuracy: {clf.score(X=X_test, y=y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6375a",
   "metadata": {},
   "source": [
    "Having concluded the training and testing, we can conclude that our classifier can predict a movie review with approximately a 90% accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataexercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
