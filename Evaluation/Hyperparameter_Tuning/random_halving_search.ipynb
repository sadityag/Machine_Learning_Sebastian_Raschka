{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b61cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670fbc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "B    357\n",
      "M    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "# Breast cancer data, with diagnosis as target variable\n",
    "print(df.loc[:, 1].value_counts())\n",
    "\n",
    "y = df.loc[:, 1].values\n",
    "X = df.loc[:, 2:].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8755940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search best accuracy score: 0.968.\n",
      "Random Search best parameters: {'svc__C': np.float64(0.4793116053425186), 'svc__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_halving_search_cv\n",
    "# Experimental - might not be supported past 1.0\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "# Grid searches literall just brute force the seach for values of hyperparameters over the specified grid\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScaler(),\n",
    "                         SVC(random_state=42))\n",
    "\n",
    "param_range = scipy.stats.loguniform(10. ** -4, 10. **3)\n",
    "param_grid = [{'svc__C': param_range,\n",
    "               'svc__kernel': ['linear']}, \n",
    "# For the linear kernel in SVC, scan over regularization strengths\n",
    "              {'svc__C': param_range,\n",
    "               'svc__gamma': param_range,\n",
    "               'svc__kernel': ['rbf']}] \n",
    "# For the RBF (Gaussian) kernel, scan over different regularlization strengths as well as different Gaussian widths\n",
    "\n",
    "hs = HalvingRandomSearchCV(estimator=pipe_svc,\n",
    "                  param_distributions=param_grid,\n",
    "                  resource='n_samples', # Training set size is considered the resource to devote more of as fewer candidates remain\n",
    "                  factor=1.5,# Keep best 1/1.5 ~ 66% of the candidates each round\n",
    "                  n_candidates='exhaust', # Find the absolute best candidate\n",
    "                  refit=True, # Refits to the whole training set automatically after finding the best one\n",
    "                  n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "print(f\"Random Search best accuracy score: {hs.best_score_:.3f}.\")\n",
    "print(f\"Random Search best parameters: {hs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7915d315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.965\n"
     ]
    }
   ],
   "source": [
    "# We can now retreive the best estimator\n",
    "classifier = hs.best_estimator_\n",
    "# classifier.fit(X_train, y_train) # Unnecessary since we set refit to True\n",
    "print(f'Test accuracy: {classifier.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a96d29",
   "metadata": {},
   "source": [
    "Other hyperparameter optimization/tuning packages exists as well, such as hyperopt (which has sklearn support https://github.com/hyperopt/hyperopt-sklearn). This has several methods included, such as TPE, which is a Bayesian optimizer than updates priors w.r.t past evaluations to make for more informed sampling of hyperparameter space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataexercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
