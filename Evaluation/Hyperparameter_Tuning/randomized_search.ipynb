{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b61a6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee860530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "B    357\n",
      "M    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "# Breast cancer data, with diagnosis as target variable\n",
    "print(df.loc[:, 1].value_counts())\n",
    "\n",
    "y = df.loc[:, 1].values\n",
    "X = df.loc[:, 2:].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50d36897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.18582273e-02 4.51856095e+02 1.33032451e+01 1.55099140e+00\n",
      " 1.23631883e-03 1.23583828e-03 2.55026485e-04 1.15673272e+02\n",
      " 1.61363417e+00 9.04707196e+00]\n"
     ]
    }
   ],
   "source": [
    "# Randomized Search can sample over discrete lists of values as param inputs\n",
    "# But it can also sample from distributions\n",
    "import scipy\n",
    "param_range = scipy.stats.loguniform(10. ** -4, 10. **3)\n",
    "# This is a log-uniform distribution, and we can sample from it using the rvs call:\n",
    "np.random.seed(42)\n",
    "print(param_range.rvs(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ee8fb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search best accuracy score: 0.969.\n",
      "Random Search best parameters: {'svc__C': np.float64(655.0713895392056), 'svc__gamma': np.float64(0.004259899791418376), 'svc__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Grid searches literall just brute force the seach for values of hyperparameters over the specified grid\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScaler(),\n",
    "                         SVC(random_state=42))\n",
    "\n",
    "param_grid = [{'svc__C': param_range,\n",
    "               'svc__kernel': ['linear']}, \n",
    "# For the linear kernel in SVC, scan over regularization strengths\n",
    "              {'svc__C': param_range,\n",
    "               'svc__gamma': param_range,\n",
    "               'svc__kernel': ['rbf']}] \n",
    "# For the RBF (Gaussian) kernel, scan over different regularlization strengths as well as different Gaussian widths\n",
    "\n",
    "rs = RandomizedSearchCV(estimator=pipe_svc,\n",
    "                  param_distributions=param_grid, # Samples distributions now, instead of a grid\n",
    "                  scoring='accuracy',\n",
    "                  cv=10,\n",
    "                  n_iter=20, # 20 samples\n",
    "                  refit=True, # Refits to the whole training set automatically after finding the best one\n",
    "                  n_jobs=-1).fit(X_train, y_train)\n",
    "\n",
    "print(f\"Random Search best accuracy score: {rs.best_score_:.3f}.\")\n",
    "print(f\"Random Search best parameters: {rs.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d853185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.951\n"
     ]
    }
   ],
   "source": [
    "# We can now retreive the best estimator\n",
    "classifier = rs.best_estimator_\n",
    "# classifier.fit(X_train, y_train) # Unnecessary since we set refit to True\n",
    "print(f'Test accuracy: {classifier.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f26a44",
   "metadata": {},
   "source": [
    "Now, a completely random search might not yield the best possible results in a fixed time. A slightly more resource efficient method would be to throw out parts of configuration space that are simply not yielding good results. Throwing out the bottom 50% of hyperparameters from a large list trained on fewer datapoints to converge quicker is known as successive halving, which has been experimentally implemented in Scikit Learn. After throwing out the bottom half, more resources (training examples) can be expended on trying to search among the top 50%. This is done recursively until the best survives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataexercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
