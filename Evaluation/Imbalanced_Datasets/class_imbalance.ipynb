{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76bae47",
   "metadata": {},
   "source": [
    "If a dataset has 90 labels being in class 1, and 10 in class 2, a classifier that always predicted class 1 would have a 90% accuracy score, despite learning almost nothing. In order to deal with this, we can balance the way we score things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f815003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dae7c32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "B    357\n",
      "M    212\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', header=None)\n",
    "# Breast cancer data, with diagnosis as target variable\n",
    "print(df.loc[:, 1].value_counts())\n",
    "\n",
    "y = df.loc[:, 1].values\n",
    "X = df.loc[:, 2:].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882c6cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy: 89.924%. Clearly, this isn't a good metric.\n"
     ]
    }
   ],
   "source": [
    "# Let's create an imbalanced dataset:\n",
    "X_imbalanced = np.vstack((X[y == 0], X[y == 1][:40]))\n",
    "y_imbalanced = np.hstack((y[y == 0], y[y == 1][:40]))\n",
    "\n",
    "# Let's try a uniform predictor that has learned only the majority class\n",
    "prediction = np.zeros(y_imbalanced.shape[0])\n",
    "print(f\"Prediction accuracy: {np.mean(y_imbalanced == prediction) * 100:.3f}%. Clearly, this isn't a good metric.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0086b972",
   "metadata": {},
   "source": [
    "There are a few ways to deal with this.\n",
    "\n",
    "1) Choosing appropriate metrics for the purpose. If you can about making sure you detect everyone with malignant tumors (even if you catch a few false positives for it) to recommend further screening, then you want to maximize the true positive rate (recall).\n",
    "\n",
    "2) Use larger penalties to wrong predictions on the minority class. With scikit-learn, this can be as simple as setting class_weight='balanced' in classifiers.\n",
    "\n",
    "3) Upsample the minority or downsample the majority to maintain an even proportion of samples for training. Scikit-learn makes this simple with the resample method, by creating a new training set with taking samples with replacement from the minority.\n",
    "\n",
    "4) Generate synthetic data for the minority class.\n",
    "\n",
    "There is no single best way - try a few things, and see how they fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f19ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial samples: [357  40]\n",
      "Post resampling: [  0 357]\n",
      "Post balancing: [357 357]\n"
     ]
    }
   ],
   "source": [
    "# Generating an upsampled dataset for the minority class the imbalanced dataset\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "print(f\"Initial samples: {np.bincount(y_imbalanced)}\")\n",
    "\n",
    "X_upsampled, y_upsampled = resample(X_imbalanced[y_imbalanced == 1], y_imbalanced[y_imbalanced == 1], # specify the part to resample from\n",
    "                                    replace=True,# Draw with replacement\n",
    "                                    n_samples=X_imbalanced[y_imbalanced == 0].shape[0],# Make as many samples as the other class\n",
    "                                    random_state=42)\n",
    "\n",
    "print(f\"Post resampling: {np.bincount(y_upsampled)}\")\n",
    "\n",
    "X_balanced = np.vstack((X[y == 0], X_upsampled))\n",
    "y_balanced = np.hstack((y[y == 0], y_upsampled))\n",
    "\n",
    "print(f\"Post balancing: {np.bincount(y_balanced)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3720869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post resampling: [40]\n",
      "Post balancing: [40 40]\n"
     ]
    }
   ],
   "source": [
    "# Downsampling the majority works similarly\n",
    "\n",
    "X_downsampled, y_downsampled = resample(X_imbalanced[y_imbalanced == 0], y_imbalanced[y_imbalanced == 0],\n",
    "                                        replace=False,# We don't need to draw with replacement since we have more than enough\n",
    "                                        n_samples=X_imbalanced[y_imbalanced==1].shape[0],\n",
    "                                        random_state=42)\n",
    "\n",
    "print(f\"Post resampling: {np.bincount(y_downsampled)}\")\n",
    "\n",
    "X_balanced = np.vstack((X_imbalanced[y_imbalanced == 1], X_downsampled))\n",
    "y_balanced = np.hstack((y_imbalanced[y_imbalanced == 1], y_downsampled))\n",
    "\n",
    "print(f\"Post balancing: {np.bincount(y_balanced)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e89ec",
   "metadata": {},
   "source": [
    "With both of the upsampled and downsampled datasets, the uniform predictor would only have an accuracy score of 50%.\n",
    "\n",
    "Finally, generating synthetic training datasets can also be taken up with other libraries. The most popular algorith mfor this is SMOTE (Synthetic Minority Oversampling TEchnique).\n",
    "SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n",
    "\n",
    "Specifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n",
    "\n",
    "There is an sklearn compatible module for it: https://github.com/scikit-learn-contrib/imbalanced-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataexercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
