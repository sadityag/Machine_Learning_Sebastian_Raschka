{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3438feca",
   "metadata": {},
   "source": [
    "The torch community has built several libraries and APIs on top of PyTorch with various additional features. Some popular ones include fastai, Catalyst, PyTorch Lightning, and PyTorch Ignite. We will explore PyTorch Lightning in this notebook, which removes the manual aspects of backprop, mixed-precision, multi-GPU support, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0163327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Auto installed with Lightning\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f1faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(pl.LightningModule):\n",
    "    def __init__(self, image_shape=(1, 28, 28), hidden_units=(32, 16)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=10)\n",
    "        self.valid_acc = Accuracy(task='multiclass', num_classes=10)\n",
    "        self.test_acc = Accuracy(task='multiclass', num_classes=10)\n",
    "\n",
    "        input_size = image_shape[0] * image_shape[1] * image_shape[2]\n",
    "        all_layers = [nn.Flatten()]\n",
    "\n",
    "        for hidden in hidden_units:\n",
    "            layer = nn.Linear(input_size, hidden)\n",
    "            all_layers.append(layer)\n",
    "            all_layers.append(nn.ReLU())\n",
    "            input_size = hidden\n",
    "        \n",
    "        all_layers.append(nn.Linear(hidden_units[-1], 10))\n",
    "        self.model = nn.Sequential(*all_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        # self here refers to the MLP instance, which runs the model on the input\n",
    "        loss = nn.functional.cross_entropy(self(x), y)\n",
    "\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.train_acc.update(preds, y)\n",
    "        \n",
    "        # log stores the loss for analysis later\n",
    "        # We don't log the accuracy until the end of an epoch\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def on_epoch_end(self, outs):\n",
    "        self.log(\"train_acc\", self.train_acc.compute())\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        # self here refers to the MLP instance, which runs the model on the input\n",
    "        loss = nn.functional.cross_entropy(self(x), y)\n",
    "\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.valid_acc.update(preds, y)\n",
    "        \n",
    "        self.log(\"valid_loss\", loss, prog_bar=True)\n",
    "        self.log(\"valid_acc\", self.valid_acc.compute(), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        # self here refers to the MLP instance, which runs the model on the input\n",
    "        loss = nn.functional.cross_entropy(self(x), y)\n",
    "\n",
    "        logits = self(x)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        self.test_acc.update(preds, y)\n",
    "        \n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", self.test_acc.compute(), prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3afc3025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data:\n",
    "# Option 1: Make the dataset part of the model (above)\n",
    "# Option 2: Set up the usual DataLoaders and feed them into the Lightning's fit function (need to create a Trainer instance)\n",
    "# Option\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_path='./'):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        MNIST(root=self.data_path, download=False)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        # stage can take 'fit', 'validate', 'test', 'predict'\n",
    "        mnist_all = MNIST(root=self.data_path, train=True, transform=self.transform, download=False)\n",
    "\n",
    "        self.train, self.val = random_split(mnist_all, [55000, 5000], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "        self.test = MNIST(root=self.data_path, train=False, transform=self.transform, download=False)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train, batch_size=64, num_workers=4)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val, batch_size=64, num_workers=4)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test, batch_size=64, num_workers=4)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "mnist_dm = MNISTDataModule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237a7e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadit\\miniconda3\\envs\\dataexercises\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadit\\miniconda3\\envs\\dataexercises\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:18<00:00, 45.74it/s, v_num=2, train_loss=0.0178, valid_loss=0.152, valid_acc=0.940] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 860/860 [00:18<00:00, 45.74it/s, v_num=2, train_loss=0.0178, valid_loss=0.152, valid_acc=0.940]\n"
     ]
    }
   ],
   "source": [
    "mnistclassifier = MLP()\n",
    "\n",
    "if torch.cuda.is_available(): # if you GPUs are available\n",
    "    trainer = pl.Trainer(max_epochs=10, accelerator='gpu')\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=10)\n",
    "\n",
    "trainer.fit(model=mnistclassifier, datamodule=mnist_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22e761df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3736), started 0:00:35 ago. (Use '!kill 3736' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1365760e40aedfb9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1365760e40aedfb9\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "063533b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at ./lightning_logs/version_2/checkpoints/epoch=9-step=8600.ckpt\n",
      "c:\\Users\\sadit\\miniconda3\\envs\\dataexercises\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:362: The dirpath has changed from 'd:\\\\Primary Storage\\\\Programming\\\\Machine_Learning_Sebastian_Raschka\\\\Neural_Networks\\\\pytroch_fundamentals\\\\lightning_logs\\\\version_2\\\\checkpoints' to 'd:\\\\Primary Storage\\\\Programming\\\\Machine_Learning_Sebastian_Raschka\\\\Neural_Networks\\\\pytroch_fundamentals\\\\lightning_logs\\\\version_5\\\\checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | train_acc | MulticlassAccuracy | 0      | train\n",
      "1 | valid_acc | MulticlassAccuracy | 0      | train\n",
      "2 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "3 | model     | Sequential         | 25.8 K | train\n",
      "---------------------------------------------------------\n",
      "25.8 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.8 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n",
      "10        Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at ./lightning_logs/version_2/checkpoints/epoch=9-step=8600.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadit\\miniconda3\\envs\\dataexercises\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sadit\\miniconda3\\envs\\dataexercises\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 860/860 [00:18<00:00, 46.11it/s, v_num=5, train_loss=0.0129, valid_loss=0.146, valid_acc=0.946] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 860/860 [00:18<00:00, 46.07it/s, v_num=5, train_loss=0.0129, valid_loss=0.146, valid_acc=0.946]\n"
     ]
    }
   ],
   "source": [
    "# We can resume training from a checkpoint in this model folder\n",
    "if torch.cuda.is_available(): # if you have GPUs\n",
    "    trainer = pl.Trainer(max_epochs=15, accelerator='gpu')\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=15)\n",
    "\n",
    "trainer.fit(model=mnistclassifier, datamodule=mnist_dm, ckpt_path='./lightning_logs/version_2/checkpoints/epoch=9-step=8600.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d5e5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 3736), started 0:13:56 ago. (Use '!kill 3736' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b3eeedc144c72941\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b3eeedc144c72941\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LEt's check if the additional training was worth it\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e54f3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "c:\\Users\\sadit\\miniconda3\\envs\\dataexercises\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:420: Consider setting `persistent_workers=True` in 'test_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 157/157 [00:00<00:00, 198.41it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9600574970245361\n",
      "        test_loss           0.12282264232635498\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.12282264232635498, 'test_acc': 0.9600574970245361}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The gain in an additional 5 epochs was under 1% - whether this is worth it is completely up to the individual and the case at hand\n",
    "trainer.test(model=mnistclassifier, datamodule=mnist_dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataexercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
